# SPOTIFY ETL DATA PIPELINE
The goal of every data project is to bring value to the business thatâ€™s why we collect different type of data so that we derive different insights from that.

We have a client who is passionate about music industry and want to understand this music industry by collecting different data so that he can find some pattern and make music base on that so on Spotify we have multiple playlist available and for starter our client want to start with the top global songs. every single week he want to collect top global song available on the bill board of Spotify so that he can understand what type of song people like and what type of song is in trending or what kind of album are trending and who is the artist making this song so all of these information we will be extracting from Spotify API.

So Spotify is music streaming app so that you can play the song you like and whatever song you can find here so that play list of top global song updates weekly so what our client wants to build a large dataset throughout the year so he want to start collecting that data on weekly basis so that after one year or two year he can find different insights based on all of these data so for that he want to build ETL pipeline or data pipeline

First we have Spotify API. This API only provided by the Spotify  so that we can scrape/extract the music data from it and make use of it, then we will use Spotify package for the extraction of data then we will deploy our code on AWS lambda which is a compute service where we can deploy our code and run it on different event bases and on top that we can apply amazon cloud watch trigger to mention that we want to run our code on daily base or weekly bases or monthly bases so we can schedule all those things using cloud watch for example we apply a trigger that this lambda function should run on daily bases so we can add some kind of trigger to this particular code to run on this particular time so this is what cloud watch does it run our code on that specific time that we provide so it run the lambda function and extract the data from Spotify API and put that on amazon s3(raw data)  so first we collect the raw data and store as it in S3. S3 is basically the object storage so that we can store different types of file so once we have our data on S3 we will add a trigger so we have already a data on S3 then we run a transformation Job  so we will also write some transformation code and again deploy on lambda function so that when we will get any data on amazon S3(raw data) it will automatically trigger the transformation function means lambda function for transformation and store that data in S3(transformed data) so that was our extract and transform jobs and it will be completely automated.

Once we have a data on S3 we have a glue crawler so crawler will go through each and every file and try to understand how many columns it has what are the different column names and what is the datatype of each columns it will understand our entire file as it is and will build a glue catalog so glue catalog is basically information about our data how many column and what are data type all will be store here in glue catalog and once we have our data in glue catalog we have amazon Athena to directly run SQL query on top of that data
